{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98003aa9-634e-4550-bea7-a51ab52ff0e5",
   "metadata": {},
   "source": [
    "# Kafka \n",
    "\n",
    "Kafka is a distributed streaming platform that is designed to handle real-time data feeds.\n",
    "\n",
    "\n",
    "\n",
    "STEPS TO START THE KAFKA ENVIRONMENT\n",
    "\n",
    "Start the ZooKeeper service: 2181\n",
    "\n",
    "- .\\bin\\windows\\zookeeper-server-start.bat .\\config\\zookeeper.properties\n",
    "\n",
    "Start the Kafka broker service: \n",
    "\n",
    "- .\\bin\\windows\\kafka-server-start.bat .\\config\\server.properties\n",
    "\n",
    "STEP 3: CREATE A TOPIC TO STORE YOUR EVENTS\n",
    "\n",
    "- .\\bin\\windows\\kafka-topics.bat --create --topic final_project_demo --bootstrap-server localhost:9092\n",
    "\n",
    "STEP 4: WRITE SOME EVENTS INTO THE TOPIC\n",
    "\n",
    "- .\\bin\\windows\\kafka-console-producer.bat --topic final_project_demo --bootstrap-server localhost:9092\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665132ce-692d-4530-a0c1-8090dc55cffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 234 ms\n",
      "Wall time: 6.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import DataFrame\n",
    "import re\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Kafka_test\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .config(\"spark.driver.memory\",\"4g\") \\\n",
    "        .config(\"spark.executor.memory\",\"5g\") \\\n",
    "        .master(\"local\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492e7c3a-98ed-4e3d-bb83-4afff06c5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf0bb648-d7fc-41f2-9b72-66b80e70a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo1\n",
      "Demo2\n",
      "Demo3\n",
      "Demo4\n"
     ]
    }
   ],
   "source": [
    "consumer = KafkaConsumer('final_demo_1', bootstrap_servers=['localhost:9092'])\n",
    "message_count = 0\n",
    "for message in consumer:\n",
    "    print(message.value.decode('utf-8'))\n",
    "    message_count += 1\n",
    "    # exit the loop after consuming 10 messages\n",
    "    if message_count >= 4:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
