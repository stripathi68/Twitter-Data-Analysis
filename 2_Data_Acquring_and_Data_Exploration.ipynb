{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fede96b1",
   "metadata": {},
   "source": [
    "# Data_Acquring_and_Data_Exploration:\n",
    "\n",
    "                                            \n",
    "# Overview\n",
    "\n",
    "Data Source: http://help.sentiment140.com/for-students\n",
    "\n",
    "Here we are using Sentiment140 datasets for Academic purpose in order to train our supervised machine learning models for performing sentimental analysis on \"Ukraine Crisis\" Dataset.\n",
    "\n",
    "We have uploaded the test and train datasets into mongoDB Compass which we obtained from \"sentiment140\" website.\n",
    "\n",
    "The data is a CSV with emoticons removed. Also, there are no null values in this dataset.\n",
    "\n",
    "Data file format has 6 fields/columns:\n",
    "\n",
    "- 0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "- 1 - the id of the tweet (2087)\n",
    "- 2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "- 3 - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "- 4 - the user that tweeted (robotickilldozr)\n",
    "- 5 - the text of the tweet (Lyx is cool)\n",
    "\n",
    "We have used magic command (%%time) in Jupyter notebooks which is used to measure the execution time of a code block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eef9d0-a918-4028-b5e2-8d49afa26814",
   "metadata": {},
   "source": [
    "# Initializing spark instance from a JupyterLab environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e92850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 7.66 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecdaa15-7ff4-443c-b4f0-e946d657d78d",
   "metadata": {},
   "source": [
    "\n",
    "# Importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c1fff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 196 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e27c355-af6e-450f-8fbb-fd5fc5b52aa5",
   "metadata": {},
   "source": [
    "# Creating a spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc289bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Data Acquring and exploration\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .config(\"spark.driver.memory\",\"40g\") \\\n",
    "        .config(\"spark.executor.memory\",\"50g\") \\\n",
    "        .master(\"local\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32118804-1a22-4587-891e-0a323d638b1c",
   "metadata": {},
   "source": [
    "# Defining mongoDB compass ip address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03badd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mongo_ip = \"mongodb://localhost:27017/streaming.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa84c8a-e0c5-4d4c-ba80-c95ff7020e0d",
   "metadata": {},
   "source": [
    "# Reading the test_nlp_data collection from the mongoDB which is in the database by the name streaming and counting number of rows in this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5b85e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3.82 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "testdf = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", mongo_ip + \"test_nlp_data\").load()\n",
    "\n",
    "testdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f40c5-a119-4264-843d-4f826102055f",
   "metadata": {},
   "source": [
    "# Applying proper schema to our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcac8878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 164 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# apply the new schema to the testdf DataFrame\n",
    "\n",
    "testdf = testdf \\\n",
    "    .withColumn(\"polarity\", col(\"_c0\").cast(\"float\")) \\\n",
    "    .withColumn(\"id\", col(\"_c1\").cast(\"long\")) \\\n",
    "    .withColumn(\"date_time\", col(\"_c2\").cast(\"string\")) \\\n",
    "    .withColumn(\"query\", col(\"_c3\").cast(\"string\")) \\\n",
    "    .withColumn(\"user\", col(\"_c4\").cast(\"string\")) \\\n",
    "    .withColumn(\"text\", col(\"_c5\").cast(\"string\")) \\\n",
    "    .drop(\"_c0\", \"_c1\", \"_c2\", \"_c3\", \"_c4\", \"_c5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c178616-f0b7-442d-ad76-80afd110979f",
   "metadata": {},
   "source": [
    "# Performing grouping operation on polarity column to see number of records for each polarity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea9a187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|polarity|count|\n",
      "+--------+-----+\n",
      "|     2.0|  139|\n",
      "|     4.0|  182|\n",
      "|     0.0|  177|\n",
      "+--------+-----+\n",
      "\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 745 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "testdf.groupBy(\"polarity\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5169b42f-87fd-4280-8858-3255058d1baf",
   "metadata": {},
   "source": [
    "\n",
    "# Reading the train_nlp_data collection from the mongoDB which is in the database by the name streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6098465-2f1c-412d-ab03-37636dc6081d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 12.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "traindf = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", mongo_ip + \"train_nlp_data\").load()\n",
    "\n",
    "traindf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7029d50-3e3a-4a61-8062-3138976d22eb",
   "metadata": {},
   "source": [
    "\n",
    "# Apply proper schema to the traindf DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19fe157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 50.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "traindf = traindf \\\n",
    "    .withColumn(\"polarity\", col(\"_c0\").cast(\"float\")) \\\n",
    "    .withColumn(\"id\", col(\"_c1\").cast(\"long\")) \\\n",
    "    .withColumn(\"date_time\", col(\"_c2\").cast(\"string\")) \\\n",
    "    .withColumn(\"query\", col(\"_c3\").cast(\"string\")) \\\n",
    "    .withColumn(\"user\", col(\"_c4\").cast(\"string\")) \\\n",
    "    .withColumn(\"text\", col(\"_c5\").cast(\"string\")) \\\n",
    "    .drop(\"_c0\", \"_c1\", \"_c2\", \"_c3\", \"_c4\", \"_c5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d10a5-b796-4e12-a5d4-7caa2f6a9f5e",
   "metadata": {},
   "source": [
    "# Creating Temp view on the train dataset in order to perform sparkSQL operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e8952b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 60.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "traindf.createOrReplaceTempView(\"traindf\")\n",
    "\n",
    "traindf_filtered = spark.sql(\"SELECT polarity,text,date_time FROM traindf;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ebb76-2ef1-4d51-9abf-25fd509f65e4",
   "metadata": {},
   "source": [
    "# Performing grouping operation on polarity column to see number of records for each polarity type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43b9aae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|polarity|count(1)|\n",
      "+--------+--------+\n",
      "|     0.0|  800000|\n",
      "|     4.0|  800000|\n",
      "+--------+--------+\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 7.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "traindf_sql = spark.sql(\"SELECT polarity,count(*) FROM traindf group by polarity;\")\n",
    "\n",
    "traindf_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159d8db-702b-402c-b450-3b61d4f8dcd0",
   "metadata": {},
   "source": [
    "# Creating a new collection in mongoDB by the name traindf_reduced for storing the values of traindf_reduced dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6381e876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "traindf_filtered.repartition(20).write \\\n",
    "  .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"uri\", mongo_ip + \"traindf_filtered\") \\\n",
    "  .option(\"partitioner\", \"MongoSinglePartitioner\") \\\n",
    "  .option(\"partitionKey\", \"polarity\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46035504",
   "metadata": {},
   "source": [
    "# Inference:\n",
    "\n",
    "We can not use test dataset since test and train datasets are different. Train data set is having 1.6M rows and the polarity is only there for negative (0) and positive(4). The more number of records we have our model will be more accurate.\n",
    "\n",
    "Since we are going to use training dataset from sentiment140 our model will be trained to detect only 0 (Negative Sentiment) and 4 (Positive Sentiment)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
