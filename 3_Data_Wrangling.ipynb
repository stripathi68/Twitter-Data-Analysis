{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b440497-2b3f-4b74-96dd-94cde582474a",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "Here we will be cleaning and reformating the twitter text column present in the traindf_filtered dataframe obtained in the previous step. Here we will be removing some unwanted elements in the text column so that it will not impact the performance of our machine learning model.\n",
    "\n",
    "We're going to have to do the following to the data in the text column:\n",
    "\n",
    "- Remove Email and URLs.\n",
    "- Extract and then remove user-names (@mentions).\n",
    "- Extract and then remove hash-tags (#hash-tag).\n",
    "- For HTML Decoding, we have created a UDF (User Defined Function).\n",
    "- Finally we have removed any null values introduced in the text column after doing the data cleaning steps mentioned above.\n",
    "\n",
    "We have used regular expressions here to perform this operations along with a UDF (User Defined Function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38336af1-f6b8-4a71-8c41-565ed3e8e827",
   "metadata": {},
   "source": [
    "# Initializing spark instance from a JupyterLab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73bdf70-1ee9-439b-89c2-bd6af6987651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 2.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1649a-8739-4f49-a55a-c901a913e031",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5697b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 249 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import DataFrame\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412037f-1ddc-4254-9d10-99a840731aaf",
   "metadata": {},
   "source": [
    "# Creating a spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c2cedf-ee31-4fc8-bd5b-c9d5184c7dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 4.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Data Wrangling\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .config(\"spark.driver.memory\",\"40g\") \\\n",
    "        .config(\"spark.executor.memory\",\"50g\") \\\n",
    "        .master(\"local\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d7ac78-fbcd-40f9-b2ae-2b13344db172",
   "metadata": {},
   "source": [
    "# For HTML Decoding, we have created a UDF (User Defined Function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab5227-fbc8-4d22-abdd-39f47993da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import html\n",
    "\n",
    "@f.udf\n",
    "def html_unescape(s: str):\n",
    "    if isinstance(s, str):\n",
    "        return html.unescape(s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa9f2b-036e-4042-858a-35bca42a2d1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Defining regular expressions for performing data wrangling tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af734c96-b608-4f64-94a9-25134392d4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "user_regex = r\"(@\\w{1,15})\"\n",
    "hashtag_regex = r\"(#\\w{1,})\"\n",
    "url_regex = r\"((https?|ftp|file):\\/{2,3})+([-\\w+&@#/%=~|$?!:,.]*)|(www.)+([-\\w+&@#/%=~|$?!:,.]*)\"\n",
    "email_regex = r\"[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{1,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390670d-ffc6-4d0a-8194-f141dd045e97",
   "metadata": {},
   "source": [
    "# Creating a function to perform Data Cleaning and Data Wrangling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff30e8b-fda7-4064-84fb-09919a66836b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def clean_data(df):\n",
    "    df = (\n",
    "        df\n",
    "        #.withColumn(\"Original_text\", f.col(\"text\")) # including original text for my reference\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), url_regex, \"\")) # replacing urls with empty string\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), email_regex, \"\")) # replacing email with empty strings\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), user_regex, \"\")) # replacing @<user_name> with empty string\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \"#\", \" \")) # replacing '#' with space\n",
    "        .withColumn(\"text\", html_unescape(f.col(\"text\")))  # removing html using UDF \n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \"[^a-zA-Z']\", \" \")) # remove all numbers\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), r'\\s{1,}', ' ')) # replace consecutive spaces (1 to any number) with a single space\n",
    "        .withColumn(\"text\", f.trim(f.col(\"text\"))) # removing leading and trailing whitespaces\n",
    "        .filter(\"text != ''\") # removing empty strings\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942403dc-8ca0-48f7-a504-42029e1c61d9",
   "metadata": {},
   "source": [
    "# Importing data from traindf_reduced collection present in the mongoDB Compass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee1b6d5-a57d-480e-8179-5abfa8764e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mongo_ip = \"mongodb://localhost:27017/streaming.\"\n",
    "\n",
    "df_raw = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", mongo_ip + \"traindf_filtered\").load()\n",
    "\n",
    "df_raw.createOrReplaceTempView(\"train_df\")\n",
    "\n",
    "df_raw = spark.sql(\"SELECT polarity,text FROM train_df;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393a6d7-5416-4408-9158-d93975ee4e35",
   "metadata": {},
   "source": [
    "# Performing the above defined cleaning tasks on the df_raw data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3f904f-5e20-4211-9da8-69cb7f3c9f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_cleaned = clean_data(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5a3e5-b417-4eab-bd1a-dc07295dbb6b",
   "metadata": {},
   "source": [
    "# Uploading the cleaned data into mongoDB Compass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b82be172-ed73-4787-a913-086032e0f41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_cleaned.write \\\n",
    "  .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "  .option(\"uri\", mongo_ip + \"df_cleaned\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"partitionKey\", \"polarity\") \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
